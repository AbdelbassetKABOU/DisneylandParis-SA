{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371693ed",
   "metadata": {},
   "source": [
    "# Présentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390332b7",
   "metadata": {},
   "source": [
    "Le présent notebook traite le coté prétraitement; on parlera de la dataset, le processus de chargement, le filtrage (stop words), ainsi que les opérations de tokénisation, racinalisation (steeming) et Lematization.\n",
    "\n",
    "En fin, on exportera le résultat dans un fichier csv qui sera par la suite utilisé pour effectuer le reste des opérations demandées (exploration, visualisation et analyse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4cf05c",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d01f3",
   "metadata": {},
   "source": [
    "##### $Les$ $imports$ $necessaires$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf872feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe94cf",
   "metadata": {},
   "source": [
    "### $Le$ $dataset$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9004b",
   "metadata": {},
   "source": [
    "Le dataset (disponible içi [1]) contient plus de 13k commentaires sur Disneyland Paris.\n",
    "\n",
    "Les colonnes sont : \n",
    "- Rating (note sur 5), \n",
    "- Year_Month (mois du commentaire), \n",
    "- Reviewer_Location (Pays d'origine du commentaire), \n",
    "- Review_Text (contenu du commentaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92786f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12732861"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On récupère le fichier\n",
    "url = 'https://assets-datascientest.s3-eu-west-1.amazonaws.com/de/total/reviews.csv'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('reviews.csv', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e77b2",
   "metadata": {},
   "source": [
    "### $Extraction$ $des$ $données$ $et$ $apperçu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79e18689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape : (13630, 4)\n",
      "Dataset overview :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year_Month</th>\n",
       "      <th>Reviewer_Location</th>\n",
       "      <th>Review_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-3</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>We've been to Disneyland Hongkong and Tokyo, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-6</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>I went to Disneyland Paris in April 2018 on Ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>What a fantastic place, the queues were decent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>Australia</td>\n",
       "      <td>We didn't realise it was school holidays when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>missing</td>\n",
       "      <td>France</td>\n",
       "      <td>A Trip to Disney makes you all warm and fuzzy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating Year_Month     Reviewer_Location  \\\n",
       "0       5     2019-3  United Arab Emirates   \n",
       "1       4     2018-6        United Kingdom   \n",
       "2       5     2019-4        United Kingdom   \n",
       "3       4     2019-4             Australia   \n",
       "4       5    missing                France   \n",
       "\n",
       "                                         Review_Text  \n",
       "0  We've been to Disneyland Hongkong and Tokyo, s...  \n",
       "1  I went to Disneyland Paris in April 2018 on Ea...  \n",
       "2  What a fantastic place, the queues were decent...  \n",
       "3  We didn't realise it was school holidays when ...  \n",
       "4  A Trip to Disney makes you all warm and fuzzy ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "print('Dataset Shape :', df.shape)\n",
    "print('Dataset overview :')\n",
    "display (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac219528",
   "metadata": {},
   "source": [
    "## La propriété Target (Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9257b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.862656\n",
      "0    0.137344\n",
      "Name: sentiment, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'] = df['Rating'] > 2.5\n",
    "df.sentiment = df.sentiment.map({True:1, False:0})\n",
    "print(df.sentiment.value_counts(normalize=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444fcda",
   "metadata": {},
   "source": [
    "## Modification des labels (par commodité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "510fbcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've been to Disneyland Hongkong and Tokyo, so far this one is the best. We're looking forward to visit the biggest one in Orlando. 1 day is not enough, it is recommended to stay in Disney Hotel   Resort so you can enjoy the fast track.. save huge amount of time.. if you're not staying there, plan and strategize your visit by getting all the fast track passes from kiosk nearby the attraction; then come back when it's time for your ride. The projection and fireworks show are out of this world!!\n"
     ]
    }
   ],
   "source": [
    "# Let us add a new column\n",
    "df['text'] = df.Review_Text\n",
    "txt = df.text[0]\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490979a",
   "metadata": {},
   "source": [
    "# Text Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3837bba",
   "metadata": {},
   "source": [
    "### $Initialisation$ $et$ $packages$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1886f19",
   "metadata": {},
   "source": [
    "La bibliothèque nltk\n",
    "\n",
    "Natural Language Tool Kit (NLTK) is a Python library to make programs that work with natural language. \n",
    "It provides a user-friendly interface to datasets that are over 50 corpora and lexical resources \n",
    "such as WordNet Word repository. The library can perform different operations such as tokenizing, \n",
    "stemming, classification, parsing, tagging, and semantic reasoning [2].\n",
    "\n",
    "Natural Language Tool Kit (NLTK) est une bibliothèque Python permettant de créer des programmes qui fonctionnent avec le langage naturel. Elle fournit une interface à des datasets avec plus de 50 corpus et ressources lexicales comme, e.g. WordNet. La bibliothèque peut effectuer différentes opérations telles que la tokénisation, la déformation, la classification, l'analyse syntaxique, le balisage et le raisonnement sémantique, etc [2].\n",
    "\n",
    "*Instalation :* \n",
    "\n",
    "`sudo pip install -U nltk to install nltk on Mac or Linux`\n",
    "\n",
    "`or run pip install nltk on your cmd.exe bash on Windows.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4726bf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/abdelbasset/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abdelbasset/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/abdelbasset/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Le Filtrage Stop Words\n",
    "# Importer stopwords de la classe nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stemming (plus de détails ci-dessous)\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Lematisation\n",
    "nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c0f80",
   "metadata": {},
   "source": [
    "### $La$ $Tokenisation$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a3d91",
   "metadata": {},
   "source": [
    "La tokénisation consiste par définition,  à découper en morceaux appelés tokens une séquence de caractères en éliminant éventuellement certains caractères comme la ponctuation [3]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb826086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation done in : 1.25 s\n",
      "First line => Size =  95\n",
      "['We', 've', 'been', 'to', 'Disneyland', 'Hongkong', 'and', 'Tokyo', 'so', 'far', 'this', 'one', 'is', 'the', 'best', 'We', 're', 'looking', 'forward', 'to', 'visit', 'the', 'biggest', 'one', 'in', 'Orlando', '1', 'day', 'is', 'not', 'enough', 'it', 'is', 'recommended', 'to', 'stay', 'in', 'Disney', 'Hotel', 'Resort', 'so', 'you', 'can', 'enjoy', 'the', 'fast', 'track', 'save', 'huge', 'amount', 'of', 'time', 'if', 'you', 're', 'not', 'staying', 'there', 'plan', 'and', 'strategize', 'your', 'visit', 'by', 'getting', 'all', 'the', 'fast', 'track', 'passes', 'from', 'kiosk', 'nearby', 'the', 'attraction', 'then', 'come', 'back', 'when', 'it', 's', 'time', 'for', 'your', 'ride', 'The', 'projection', 'and', 'fireworks', 'show', 'are', 'out', 'of', 'this', 'world']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13630"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_tokenize(txt):\n",
    "    mots = word_tokenize(txt)\n",
    "    return mots\n",
    "\n",
    "def regex_tokenize(txt):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    mots = tokenizer.tokenize(txt)    \n",
    "    return mots\n",
    "\n",
    "def regex_delete_numeric(txt):\n",
    "    tokenizer = RegexpTokenizer(r'[^\\d]+')\n",
    "    mots = tokenizer.tokenize(txt)    \n",
    "    return mots\n",
    "\n",
    "start = time.time()\n",
    "mega_mots = list(map(regex_tokenize, df.text))\n",
    "end = time.time()\n",
    "print('Calculation done in :', round(end - start, 2), 's') # ==> 1.37s 1.24 1.33 1.24 1.25\n",
    "print('First line => Size = ', len(mega_mots[0]))\n",
    "print(mega_mots[0])\n",
    "len(mega_mots) # ==> 13630"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36bf16c",
   "metadata": {},
   "source": [
    "### $Le$ $Filtrage$ $Stop$ $Words$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "345fdfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after : 1.39 s\n",
      "First line => Size =  30\n",
      "['hongkong', 'tokyo', 'far', 'best', 'looking', 'forward', 'biggest', 'orlando', 'enough', 'recommended', 'stay', 'resort', 'enjoy', 'fast', 'track', 'save', 'huge', 'amount', 'staying', 'strategize', 'getting', 'fast', 'track', 'passes', 'kiosk', 'nearby', 'ride', 'projection', 'fireworks', 'show']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13630"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialiser la variable des mots vides\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ajouter plus de mots pour ne garder que des mots significatives dans notre context\n",
    "extention = {'pass', 'also', 'see', 'take', 'get', 'paris', 'park', 'find', 'still', 'even', 'staff', \\\n",
    "             'u', 'year', 'although', 'experience', 'use', 'really', 'place', 'hotel', 'visit', 'etc', 'thing', \\\n",
    "             'would', 'could', 'though', 'disneyland', 'make', 'however', 'people', 'around', 'us', 'look',\n",
    "             'know', 'line', 'think', 'theme', 'things', 'character', 'say', 'disney', 'ticket', 'many', 'lot', \\\n",
    "             'one', 'euro', 'restaurant', 'everything', 'always', 'hour', 'first', 'studio', 'tell', 'time',\\\n",
    "             'another', 'back', 'give', 'adult', 'children', 'daughter', 'work', 'two', 'go', 'open', 'village',\\\n",
    "             'come', 'mean', 'meet', 'try', 'especially', 'food', 'rid', 'kid', 'rid', 'way', 'start', 'spend', \\\n",
    "             'book', 'seem', 'plan', 'hours', 'studios', 'day', 'manage', 'minute', 'family', 'trip', 'return',\\\n",
    "             'whole', 'arrive', 'florida', 'different', 'area', 'restaurants', 'last', 'child',\\\n",
    "             'attraction', 'queue', 'days', 'eat', 'world'}\n",
    "\n",
    "extended_sw = stop_words.copy()\n",
    "extended_sw.update(extention)\n",
    "#print(sorted(c))\n",
    "\n",
    "def stop_words_filetering(list, stop_words):\n",
    "    cleaned_list = [word.lower() for word in list if word.lower() not in stop_words]\n",
    "    return cleaned_list\n",
    "\n",
    "def delete_numeric(list):\n",
    "    cleaned_list = [word.lower() for word in list if not word.lower().isdigit()]\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "mega_mots = list(map(lambda m: stop_words_filetering(m, extended_sw), mega_mots))                \n",
    "mega_words = list(map(delete_numeric, mega_mots))                 \n",
    "end = time.time()\n",
    "\n",
    "# ----------------\n",
    "# Check output\n",
    "print('Results after :', round(end - start, 2), 's') # ==> 1.51s/1.04s/1.25/1.86/1.26/1.29/1.39\n",
    "print('First line => Size = ', len(mega_words[0]))\n",
    "print(mega_words[0])\n",
    "len(mega_words) # ==> 13630\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2882e",
   "metadata": {},
   "source": [
    "### $La$ $Racinisation$ $(Stemming)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17bcf5",
   "metadata": {},
   "source": [
    "La racinisation est en fait, le procédé de transformation des flexions en leur radical ou racine. La racine\n",
    "d’un mot correspond à la partie du mot restante une fois que l’on a supprimé son (ses) préfixe(s) et suffixe(s), \n",
    "à savoir son radical [4].\n",
    "\n",
    "*`Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root \n",
    "form—generally a written word form [3]`*\n",
    "\n",
    "Il existe des Stemmers anglais et non-anglais disponibles dans le paquetage nltk [6]. Pour l'anglais, nous pouvons choisir entre **PorterStammer** et **LancasterStammer**, PorterStemmer étant le plus ancien, développé à l'origine en 1979. LancasterStemmer a été développé en 1990 et utilise une approche plus agressive que l'algorithme de Porter. Dans ce qui suit, nous optons pour **LancasterStammer** [6].\n",
    "\n",
    "**$n.b.$** On peut générer son propre ensemble de règles pour n'importe quelle langue, c'est pourquoi Python nltk a introduit les **Snowball Stemmers** qui sont utilisés pour créer des stemmers non anglais [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "358a38b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation done in : 33.13 s\n",
      "First line => Size =  30\n",
      "['hongkong', 'tokyo', 'far', 'best', 'look', 'forward', 'biggest', 'orlando', 'enough', 'recommend', 'stay', 'resort', 'enjoy', 'fast', 'track', 'sav', 'hug', 'amount', 'stay', 'strategize', 'get', 'fast', 'track', 'pass', 'kiosk', 'nearby', 'rid', 'project', 'firework', 'show']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13630"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def steem_it(words) :\n",
    "    #stemmer = EnglishStemmer()\n",
    "    stemmer = LancasterStemmer()\n",
    "    results = [stemmer.stem(word) for word in words]\n",
    "    return results\n",
    "    \n",
    "start = time.time()\n",
    "mega_steemed_words = list(map(steem_it, mega_words))                 \n",
    "end = time.time()\n",
    "\n",
    "# ----------------\n",
    "# Check output\n",
    "print('Calculation done in :', round(end - start, 2), 's') # ==> 21s/19.11s/33.9s/31.47s/33.13s\n",
    "print('First line => Size = ', len(mega_steemed_words[0]))\n",
    "print(mega_steemed_words[0])\n",
    "len(mega_steemed_words) # ==> 13630\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376e722",
   "metadata": {},
   "source": [
    "### $La$ $Lemmatisation$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6fdcf",
   "metadata": {},
   "source": [
    "La lemmatisation désigne un traitement lexical apporté à un texte en vue de son analyse. Ce traitement consiste à appliquer aux occurrences des lexèmes sujets à flexion (en français, verbes, substantifs, adjectifs) un codage renvoyant à leur entrée lexicale commune (« forme canonique » enregistrée dans les dictionnaires de la langue, le plus couramment), que l'on désigne sous le terme de lemme [7].\n",
    "\n",
    "*`Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form [6].`*\n",
    "\n",
    "NLTK fournis **WordNet** un lemmatizer qui permet d'utiliser la base de données WordNet pour rechercher des lemmes de mots.\n",
    "\n",
    "Attention au téléchargement de wordnet, nécéssaire avant d'utiliser le Lemmatizer, i.e.\n",
    "\n",
    "`nltk.download('wordnet')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83039ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation done in : 10.33 s\n",
      "First line => Size =  30\n",
      "['hongkong', 'tokyo', 'far', 'best', 'look', 'forward', 'biggest', 'orlando', 'enough', 'recommend', 'stay', 'resort', 'enjoy', 'fast', 'track', 'save', 'huge', 'amount', 'stay', 'strategize', 'get', 'fast', 'track', 'pass', 'kiosk', 'nearby', 'ride', 'projection', 'fireworks', 'show']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13630"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_it(words) :\n",
    "    results = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return results\n",
    "    \n",
    "start = time.time()\n",
    "mega_lemmatized_words = list(map(lemmatize_it, mega_words))\n",
    "end = time.time()\n",
    "\n",
    "# ----------------\n",
    "# Check output\n",
    "print('Calculation done in :', round(end - start, 2), 's') # ==> 11s/9.99/11.6s/9./6.58s/10.33s\n",
    "print('First line => Size = ', len(mega_lemmatized_words[0]))\n",
    "print(mega_lemmatized_words[0])\n",
    "len(mega_lemmatized_words) # ==> 13630\n",
    "# --------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963abd00",
   "metadata": {},
   "source": [
    "#### $Stemming$ $vs$ $Lematization$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076074cd",
   "metadata": {},
   "source": [
    "Contrairement au $lemme$ qui correspond à un terme issu de l’usage ordinaire des locuteurs de la langue, la racine $(stem)$ ne correspond généralement qu’à un terme résultant de ce type d’analyse. Par exemple, le mot $chercher$ a pour radical $cherch$ qui ne correspond pas à un terme employé en dehors d’une référence à ce radical même. Dans des cas particuliers, le radical peut coïncider avec un terme de vocabulaire ordinaire. C’est par exemple le cas de $frontal$ qui donne la racine $front$ [9]. \n",
    "\n",
    "Du coté choix, l'utilisation du stemming ou du la lemmatisation dépend fortement de nos besoins spécifiques [10]. En général, les avantages du stemming sont qu'il est simple à mettre en œuvre et rapide à exécuter. La contrepartie est que le résultat peut contenir des inexactitudes, même si elles ne sont pas pertinentes pour certaines tâches, e.g indexation de textes.\n",
    "\n",
    "La lemmatisation par contre, fournit de meilleurs résultats en effectuant une analyse qui produit de vrais mots de dictionnaire. L'opération est par conséquent plus difficile à mettre en œuvre et plus lente que le stemming. \n",
    "\n",
    "En résumé, la lemmatisation est presque toujours un meilleur choix d'un point de vue qualitatif. Avec les ressources informatiques actuelles, l'exécution d'algorithmes de lemmatisation ne devrait pas avoir d'impact significatif sur les performances globales. Cependant, si nous optimisons fortement la vitesse, un algorithme de lemmatisation plus simple peut être une possibilité [10]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823c893",
   "metadata": {},
   "source": [
    "### $Reconversion$ $vers$ $du$ $text$ $à$ $nouveau$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a0492",
   "metadata": {},
   "source": [
    "Après la normalisation, on revient vers la forme textuelle brute avant d\\'exporter notre dataframe vers un csv standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da7af87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_2_str(mylist):\n",
    "    # Définir la variable text\n",
    "    text = \"\"\n",
    "    for word in mylist : \n",
    "        text += word+ ' '\n",
    "    return text\n",
    "\n",
    "df.text = list(map(lambda alist : ' '.join(alist), mega_lemmatized_words))\n",
    "#mega_mots = list(map(list_2_str, mega_lemmatized_words))\n",
    "#mega_mots[0]\n",
    "#df.text = mega_mots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c2fab",
   "metadata": {},
   "source": [
    "### $Sauvgarde$ $et$ $exportation$ $vers$ $du$ $csv$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9fc748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year_Month</th>\n",
       "      <th>Reviewer_Location</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-3</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>1</td>\n",
       "      <td>hongkong tokyo far best look forward biggest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-6</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1</td>\n",
       "      <td>go april easter weekend say june choose date l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1</td>\n",
       "      <td>fantastic queue decent best apparently manage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1</td>\n",
       "      <td>realise school holiday go consequently extreme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>missing</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>make warm fuzzy actual big make fun fill happy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating Year_Month     Reviewer_Location  sentiment  \\\n",
       "0       5     2019-3  United Arab Emirates          1   \n",
       "1       4     2018-6        United Kingdom          1   \n",
       "2       5     2019-4        United Kingdom          1   \n",
       "3       4     2019-4             Australia          1   \n",
       "4       5    missing                France          1   \n",
       "\n",
       "                                                text  \n",
       "0  hongkong tokyo far best look forward biggest o...  \n",
       "1  go april easter weekend say june choose date l...  \n",
       "2  fantastic queue decent best apparently manage ...  \n",
       "3  realise school holiday go consequently extreme...  \n",
       "4  make warm fuzzy actual big make fun fill happy...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[['text', 'sentiment']].head()\n",
    "df.drop(['Review_Text'], axis=1, inplace=True)\n",
    "# Sauvegarde du dataset\n",
    "df.to_csv('encoded_reviews.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ff0e4",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb0c7d",
   "metadata": {},
   "source": [
    "- **[1]  :** https://assets-datascientest.s3-eu-west-1.amazonaws.com/de/total/reviews.csv\n",
    "- **[2]  :** https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "- **[3]  :** https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "\n",
    "- **[4]  :** https://fr.wikipedia.org/wiki/Racinisation\n",
    "- **[5]  :** https://en.wikipedia.org/wiki/Stemming\n",
    "- **[6]  :** https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "- **[7]  :** https://fr.wikipedia.org/wiki/Lemmatisation\n",
    "- **[8]  :** https://en.wikipedia.org/wiki/Lemmatisation\n",
    "\n",
    "- **[9]  :** https://fr.wikipedia.org/wiki/Lemmatisation \n",
    "\n",
    "- **[10] :** https://www.baeldung.com/cs/stemming-vs-lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
